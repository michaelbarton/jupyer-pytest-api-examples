{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import typing\n",
    "import textwrap\n",
    "\n",
    "import pandas\n",
    "import pytest\n",
    "import ipytest\n",
    "import _pytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "def check_input_file(input_file: pathlib.Path) -> None:\n",
    "    if not input_file.is_file():\n",
    "        raise FileNotFoundError(\"\")\n",
    "\n",
    "def run_cli(input_file: pathlib.Path) -> None:\n",
    "    \"\"\"Helper to simulate running a cli tool.\"\"\"\n",
    "    try:\n",
    "        check_input_file(input_file)\n",
    "    except FileNotFoundError:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def long_running_computation() -> typing.Tuple[pathlib.Path, pathlib.Path]:\n",
    "    \"\"\"Helper method to generate some example pandas data\"\"\"\n",
    "\n",
    "    raw_collected_data = pandas.DataFrame(\n",
    "        {\n",
    "            \"sample_id\": [1, 1, 2, 2, 1, 1, 2, 2],\n",
    "            \"measurement\": [0.1, 0.09, 2, 2.3, 5, 4.8, 7.2, 8.3],\n",
    "            \"test_variable\": [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Here's the raw output\n",
    "    raw_data_file = tmp_path / \"raw_data.csv\"\n",
    "    raw_data_file.write_text(raw_collected_data.to_csv())\n",
    "\n",
    "    # Here's some computation on the raw output\n",
    "    averages_data_file = tmp_path / \"sample_averages.csv\"\n",
    "    averages_data_file.write_text(\n",
    "        raw_collected_data.groupby([\"sample_id\"]).agg(\"mean\").to_csv()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_file_from_s3() -> pathlib.Path:\n",
    "    \"\"\"Simulate fetching a very large file from s3 that takes a while to download.\"\"\"\n",
    "    print(\"Fetching a large file from S3.\")\n",
    "    _, loc = tempfile.mkstemp()\n",
    "    return pathlib.Path(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "kind: article title: Pytest API Examples date: \"2021-01-04 00:00 GMT\"\n",
    "\n",
    "---\n",
    "\n",
    "I use pytest in most python projects, and I've had a feeling that I haven't been\n",
    "been using most of the features it provides, since I tend to only use\n",
    "`@pytest.mark` from the API. I spent some time reading through the pytest\n",
    "documentation and playing with some examples in the source [pytest jupyter\n",
    "notebook][] to get more familiarity with what's possible. After a few hours of\n",
    "playing around with pytest, realised there was a lot of functionality I was\n",
    "missing. Read below for some of the examples. Most of this is just a\n",
    "reconstitution of whats in the [pytest documentation][] for my own\n",
    "self-learning. Additionally there's useful videos, and plugins on the [awesome\n",
    "pytest][] GitHub repository.\n",
    "\n",
    "[pytest jupyter notebook]:\n",
    "  https://github.com/michaelbarton/jupyer-pytest-api-examples\n",
    "[pytest documentation]: https://docs.pytest.org/en/stable/example/index.html\n",
    "[awesome pytest]: https://github.com/augustogoulart/awesome-pytest\n",
    "\n",
    "## Pytest Fixtures\n",
    "\n",
    "Fixtures are a large part of the pytest API, and the part I was least familiar\n",
    "with. Fixtures are used in pytest tests by including them as parameters to test\n",
    "functions. The pytest API comes with a few builtin fixtures: useful ones for\n",
    "temporary files are `[tmp_path][]` and `[tmp_path_factory][]` shown below.\n",
    "\n",
    "[tmp_path_factory]:\n",
    "  https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "def test_with_tmp_path(tmp_path: pathlib.Path):\n",
    "    \"\"\"The `tmp_path` fixture provides a temporary directory.\"\"\"\n",
    "    assert tmp_path.is_dir()\n",
    "\n",
    "def test_with_tmp_path_factory(tmp_path_factory: pytest.TempPathFactory):\n",
    "    \"\"\"The `tmp_path_factory` fixture provides a factory for directories.\"\"\"\n",
    "    assert tmp_path_factory.mktemp(\"temp_dir\").is_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using fixtures to teardown\n",
    "\n",
    "Documentation: [pytest fixture][], [fixture finalization][]\n",
    "\n",
    "If you want to do clean up on the fixture after it's used, you can use `yield`\n",
    "instead of `return`. The fixture will then run the code defined after the\n",
    "`yield` statement, after the fixture-using test returns.\n",
    "\n",
    "[fixture finalization]:\n",
    "  https://docs.pytest.org/en/latest/fixture.html#teardown-cleanup-aka-fixture-finalization\n",
    "[pytest fixture]:\n",
    "  https://docs.pytest.org/en/latest/reference.html#pytest-fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmpfsgb870s\n",
      "Fetching a large file from S3.\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp4_k7d8wn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\n",
    "    \"\"\"Yield a large file, then delete it after each test completes.\"\"\"\n",
    "    large_data_file = fetch_file_from_s3()\n",
    "    yield large_data_file\n",
    "    print(\"Cleaning up file: {}\".format(large_data_file))\n",
    "    large_data_file.unlink()\n",
    "\n",
    "\n",
    "def test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\n",
    "    assert example_data_file_with_teardown.exists()\n",
    "\n",
    "\n",
    "def test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\n",
    "    assert example_data_file_with_teardown.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping fixtures\n",
    "\n",
    "Documentation: [scope sharing][]\n",
    "\n",
    "[scope sharing]:\n",
    "  https://docs.pytest.org/en/latest/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\n",
    "\n",
    "In the example above the code after the `yield` runs every time the fixture is\n",
    "used, this might be inappropriate if the fixture computationally expensive. An\n",
    "alternative to caching the result (described below), would be to set the scope\n",
    "of the fixture with `pytest.fixture(scope=...)`. For example\n",
    "`pytest.fixture(scope=\"session\")` will run only once for the entire pytest\n",
    "session. Possible values for `scope=...` are\n",
    "`[\"class\", \"module\", \"package\", \"session\"]`. A `Callable` can also be passed\n",
    "which will be evaluated once, see [dynamic scope][].\n",
    "\n",
    "[dynamic scope]: https://docs.pytest.org/en/latest/fixture.html#dynamic-scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      "Running test 1\n",
      ".Running test 2\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmpmdzgjvda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\n",
    "    \"\"\"Yield a large file, then delete it after the test completes.\"\"\"\n",
    "    large_data_file = fetch_file_from_s3()\n",
    "    yield large_data_file\n",
    "    print(\"Cleaning up file: {}\".format(large_data_file))\n",
    "    large_data_file.unlink()\n",
    "\n",
    "\n",
    "def test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\n",
    "    print(\"Running test 1\")\n",
    "    assert example_data_file_for_session.exists()\n",
    "\n",
    "\n",
    "def test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\n",
    "    print(\"Running test 2\")\n",
    "    assert example_data_file_for_session.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterising fixtures\n",
    "\n",
    "If you find yourself using the same `pytest.mark.parametrize` multiple times in\n",
    "your tests, this can be refactored into a fixture using\n",
    "`pytest.fixture(params=...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test passes checking for input file: .\n",
      ".Unit test passes checking for input file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp3o8iyrp0\n",
      ".Unit test passes checking for input file: /non_existing_file\n",
      ".CLI test passes checking for file: .\n",
      ".CLI test passes checking for file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp3o8iyrp0\n",
      ".CLI test passes checking for file: /non_existing_file\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture(params=[\"\", tempfile.mkdtemp(), \"/non_existing_file\"])\n",
    "def invalid_file(request) -> pathlib.Path:\n",
    "    return pathlib.Path(request.param)\n",
    "\n",
    "\n",
    "def test_file_1(invalid_file):\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        check_input_file(invalid_file)\n",
    "    print(\"Unit test passes checking for input file: {}\".format(invalid_file))\n",
    "\n",
    "\n",
    "def test_cli_app(invalid_file):\n",
    "    assert run_cli(invalid_file) == 1\n",
    "    print(\"CLI test passes checking for file: {}\".format(invalid_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break up expensive serial tests\n",
    "\n",
    "There can be scenarios in end to end tests where it's necessary to test the\n",
    "output artefact with multiple assertions. An example of this might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "def test_long_e2e_test(tmp_path: pathlib.Path):\n",
    "    \"\"\"Long running e2e test.\"\"\"\n",
    "\n",
    "    # Assume this data was generated from an expensive computation that takes a few\n",
    "    # minutes to run each time.\n",
    "    raw_data_file, averages_data_file = long_running_computation()\n",
    "\n",
    "    # If these tests fail ...\n",
    "    assert raw_data_file.exists()\n",
    "    assert raw_data_file.read_text()\n",
    "\n",
    "    # ... these won't be executed.\n",
    "    # Which can be brittle and need multiple iterations before all assertions are run.\n",
    "    assert averages_data_file.exists()\n",
    "    assert averages_data_file.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with test structure above is that running a lot of tests in serial\n",
    "means the later tests won't execute if any of the earlier ones fail which can\n",
    "require running the same tests multiple times until all the serial tests\n",
    "execute. These can instead be rewritten to take advantage of fixtures and still\n",
    "run all the tests even if some fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "# Move the long running code into a fixture and make sure it runs only once per\n",
    "# testing session\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def computation_data(\n",
    "    tmp_path_factory: pytest.TempPathFactory,\n",
    ") -> typing.Dict[str, pathlib.Path]:\n",
    "    # This data was generated by a long compuation.\n",
    "    raw_collected_data = long_running_computation()\n",
    "\n",
    "    # tmp_path_factory is a fixture provided by pytest:\n",
    "    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\n",
    "    tmp_path = tmp_path_factory.mktemp(\"e2e_test\")\n",
    "\n",
    "    # Same generated output files\n",
    "    raw_file = tmp_path / \"raw_data.csv\"\n",
    "    raw_file.write_text(raw_collected_data.to_csv())\n",
    "    averages_file = tmp_path / \"sample_averages.csv\"\n",
    "    averages_file.write_text(\n",
    "        raw_collected_data.groupby([\"sample_id\"]).agg(\"mean\").to_csv()\n",
    "    )\n",
    "\n",
    "    # Return the files for testing.\n",
    "    return {\"raw\": raw_file, \"averages\": averages_file}\n",
    "\n",
    "\n",
    "# Both these tests use the compuation data as a fixture.\n",
    "# Which means if either test fails, the other tests will still run.\n",
    "# This can also make the tests more modular and easy to read.\n",
    "\n",
    "\n",
    "def test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\n",
    "    raw_data_file = computation_data[\"raw\"]\n",
    "    assert raw_data_file.exists()\n",
    "    assert raw_data_file.read_text()\n",
    "\n",
    "\n",
    "def test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\n",
    "    averages_file = computation_data[\"averages\"]\n",
    "    assert averages_file.exists()\n",
    "    assert averages_file.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LineMatcher for testing large text\n",
    "\n",
    "The `LineMatcher` helper class provides methods that can reduce boiler plate\n",
    "testing large blocks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --quiet\n",
    "\n",
    "\n",
    "def test_large_text():\n",
    "    example_text = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        Two roads diverged in a yellow wood,\n",
    "        And sorry I could not travel both\n",
    "        And be one traveler, long I stood\n",
    "        And looked down one as far as I could\n",
    "        To where it bent in the undergrowth;\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\n",
    "\n",
    "    # Check some lines at random\n",
    "    matcher.fnmatch_lines_random([\"Two roads diverged in a yellow wood,\"])\n",
    "\n",
    "    # Check lines exist with a regex\n",
    "    matcher.re_match_lines_random([\"Two roads diverged in a .* wood,\"])\n",
    "\n",
    "    # Check lines don't exist with a regex\n",
    "    matcher.no_fnmatch_line(\"And looked down two as far as I could\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching large files or computation\n",
    "\n",
    "Documentation: [Cache config][cache]\n",
    "\n",
    "Pytest allows caching expensive operations between test runs such as large\n",
    "computation or fetching data. This can used prevent expensive computations from\n",
    "slowing down tests. The cache can be cleared using the flag:\n",
    "`pytest --cache-clear`.\n",
    "\n",
    "To access the cache the `pytestconfig` fixture needs to be in arguments to a\n",
    "fixture, this will be an instance of [`_pytest.config.Config`][config_class].\n",
    "The caveat to using the `get/set` methods is they have to be JSON serialisable,\n",
    "so in the examples below I covert `pathlib.Path` objects back and forth to\n",
    "strings to serialise into the cache.\n",
    "\n",
    "[cache]:\n",
    "  https://docs.pytest.org/en/stable/cache.html#the-new-config-cache-object\n",
    "[config_class]: https://docs.pytest.org/en/latest/reference.html#id35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      "Running test 1\n",
      ".Using cached version of file.\n",
      "Running test 2\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\n",
    "    \"\"\"Fetch and cache a large file from s3.\n",
    "\n",
    "    Notes:\n",
    "        If the file is in the cache, return it. If it's not in the cache,\n",
    "        then fetch it, cache it, then return it. This will be cached across\n",
    "        multiple testing sessions.\n",
    "\n",
    "    \"\"\"\n",
    "    if not (data_file := pytestconfig.cache.get(\"file_key\", None)):\n",
    "        data_file = fetch_file_from_s3()\n",
    "        pytestconfig.cache.set(\"file_key\", str(data_file))\n",
    "    else:\n",
    "        print(\"Using cached version of file.\")\n",
    "\n",
    "    return pathlib.Path(data_file)\n",
    "\n",
    "\n",
    "def test_file_1(example_data_file: pathlib.Path):\n",
    "    \"\"\"This test will use the non-cached version.\"\"\"\n",
    "    print(\"Running test 1\")\n",
    "    assert example_data_file.exists()\n",
    "\n",
    "\n",
    "def test_file_2(example_data_file: pathlib.Path):\n",
    "    \"\"\"Second time around this will use the cached version.\"\"\"\n",
    "    print(\"Running test 2\")\n",
    "    assert example_data_file.exists()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
