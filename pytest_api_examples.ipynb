{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import pathlib\\nimport tempfile\\nimport typing\\nimport textwrap\\n\\nimport pandas\\nimport pytest\\nimport ipytest\\nimport _pytest\\n\\nipytest.autoconfig()\\n\\n\\ndef check_input_file(input_file: pathlib.Path) -> None:\\n    if not input_file.is_file():\\n        raise FileNotFoundError(\\\"\\\")\\n        \\ndef run_cli(input_file: pathlib.Path) -> None:\\n    \\\"\\\"\\\"Helper to simulate running a cli tool.\\\"\\\"\\\"\\n    try:\\n        check_input_file(input_file)\\n    except FileNotFoundError:\\n        return 1\\n    return 0\\n\\ndef long_running_computation():\\n    \\\"\\\"\\\"Helper method to generate some example pandas data\\\"\\\"\\\"\\n    return pandas.DataFrame({\\n        \\\"sample_id\\\": [1, 1, 2, 2, 1, 1, 2, 2],\\n        \\\"measurement\\\": [0.1, 0.09, 2, 2.3, 5, 4.8, 7.2, 8.3],\\n        \\\"test_variable\\\": [\\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"B\\\", \\\"B\\\", \\\"B\\\", \\\"B\\\"]      \\n    })\\n\\ndef fetch_file_from_s3() -> pathlib.Path:\\n    \\\"\\\"\\\"Simulate fetching a very large file from s3 that takes a while to download.\\\"\\\"\\\"\\n    print(\\\"Fetching a large file from S3.\\\")\\n    _, loc = tempfile.mkstemp()\\n    return pathlib.Path(loc)\";\n",
       "                var nbb_formatted_code = \"import pathlib\\nimport tempfile\\nimport typing\\nimport textwrap\\n\\nimport pandas\\nimport pytest\\nimport ipytest\\nimport _pytest\\n\\nipytest.autoconfig()\\n\\n\\ndef check_input_file(input_file: pathlib.Path) -> None:\\n    if not input_file.is_file():\\n        raise FileNotFoundError(\\\"\\\")\\n\\n\\ndef run_cli(input_file: pathlib.Path) -> None:\\n    \\\"\\\"\\\"Helper to simulate running a cli tool.\\\"\\\"\\\"\\n    try:\\n        check_input_file(input_file)\\n    except FileNotFoundError:\\n        return 1\\n    return 0\\n\\n\\ndef long_running_computation():\\n    \\\"\\\"\\\"Helper method to generate some example pandas data\\\"\\\"\\\"\\n    return pandas.DataFrame(\\n        {\\n            \\\"sample_id\\\": [1, 1, 2, 2, 1, 1, 2, 2],\\n            \\\"measurement\\\": [0.1, 0.09, 2, 2.3, 5, 4.8, 7.2, 8.3],\\n            \\\"test_variable\\\": [\\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"B\\\", \\\"B\\\", \\\"B\\\", \\\"B\\\"],\\n        }\\n    )\\n\\n\\ndef fetch_file_from_s3() -> pathlib.Path:\\n    \\\"\\\"\\\"Simulate fetching a very large file from s3 that takes a while to download.\\\"\\\"\\\"\\n    print(\\\"Fetching a large file from S3.\\\")\\n    _, loc = tempfile.mkstemp()\\n    return pathlib.Path(loc)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import typing\n",
    "import textwrap\n",
    "\n",
    "import pandas\n",
    "import pytest\n",
    "import ipytest\n",
    "import _pytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "def check_input_file(input_file: pathlib.Path) -> None:\n",
    "    if not input_file.is_file():\n",
    "        raise FileNotFoundError(\"\")\n",
    "        \n",
    "def run_cli(input_file: pathlib.Path) -> None:\n",
    "    \"\"\"Helper to simulate running a cli tool.\"\"\"\n",
    "    try:\n",
    "        check_input_file(input_file)\n",
    "    except FileNotFoundError:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def long_running_computation():\n",
    "    \"\"\"Helper method to generate some example pandas data\"\"\"\n",
    "    return pandas.DataFrame({\n",
    "        \"sample_id\": [1, 1, 2, 2, 1, 1, 2, 2],\n",
    "        \"measurement\": [0.1, 0.09, 2, 2.3, 5, 4.8, 7.2, 8.3],\n",
    "        \"test_variable\": [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"]      \n",
    "    })\n",
    "\n",
    "def fetch_file_from_s3() -> pathlib.Path:\n",
    "    \"\"\"Simulate fetching a very large file from s3 that takes a while to download.\"\"\"\n",
    "    print(\"Fetching a large file from S3.\")\n",
    "    _, loc = tempfile.mkstemp()\n",
    "    return pathlib.Path(loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"#\";\n",
       "                var nbb_formatted_code = \"#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using fixtures to teardown\n",
    "\n",
    "Documentation: [pytest fixture][], [fixture finalization][]\n",
    "\n",
    "[fixture finalization]: https://docs.pytest.org/en/latest/fixture.html#teardown-cleanup-aka-fixture-finalization\n",
    "\n",
    "\n",
    "If you want to do clean up on the fixture you can use `yield` instead of `return`. The fixture will then run which ever code is defined after the `yield`.\n",
    "\n",
    "\n",
    "[pytest_fixture]: https://docs.pytest.org/en/latest/reference.html#pytest-fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture\\ndef example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after each test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\\n    \\ndef test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture\\ndef example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after each test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\\n    \\ndef test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp_r89vbh5\n",
      "Fetching a large file from S3.\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp4_r33rru\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture\\ndef example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after each test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\\n    \\ndef test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture\\ndef example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after each test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\\n    \\ndef test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\\n    assert example_data_file_with_teardown.exists()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "@pytest.fixture\n",
    "def example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\n",
    "    \"\"\"Yield a large file, then delete it after each test completes.\"\"\"\n",
    "    large_data_file = fetch_file_from_s3()\n",
    "    yield large_data_file\n",
    "    print(\"Cleaning up file: {}\".format(large_data_file))\n",
    "    large_data_file.unlink()\n",
    "    \n",
    "    \n",
    "def test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\n",
    "    assert example_data_file_with_teardown.exists()\n",
    "    \n",
    "def test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\n",
    "    assert example_data_file_with_teardown.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping fixtures\n",
    "\n",
    "Documentation: [scope sharing][]\n",
    "\n",
    "[scope sharing]: https://docs.pytest.org/en/latest/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\n",
    "\n",
    "In the example above the fixture teardown runs every time the fixture is used, this might be inappropriate if the fixture computationally expensive. An alternative to caching described above, would be to set the scope of the fixture with `pytest.fixture(scope=...)`. For example `pytest.fixture(scope=\"session\")` will run once for the pytest session. The values for `scope=...` are `[\"class\", \"module\", \"package\", \"session\"]`. A `Callable` can also be passed which will be evaluated once, see [dynamic scope].\n",
    "\n",
    "[dynamic scope]: https://docs.pytest.org/en/latest/fixture.html#dynamic-scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after the test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 1\\\")\\n    assert example_data_file_for_session.exists()\\n    \\ndef test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 2\\\")\\n    assert example_data_file_for_session.exists()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after the test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 1\\\")\\n    assert example_data_file_for_session.exists()\\n    \\ndef test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 2\\\")\\n    assert example_data_file_for_session.exists()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      "Running test 1\n",
      ".Running test 2\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp7z4sej7j\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after the test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 1\\\")\\n    assert example_data_file_for_session.exists()\\n    \\ndef test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 2\\\")\\n    assert example_data_file_for_session.exists()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\\n    \\\"\\\"\\\"Yield a large file, then delete it after the test completes.\\\"\\\"\\\"\\n    large_data_file = fetch_file_from_s3()\\n    yield large_data_file\\n    print(\\\"Cleaning up file: {}\\\".format(large_data_file))\\n    large_data_file.unlink()\\n    \\n    \\ndef test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 1\\\")\\n    assert example_data_file_for_session.exists()\\n    \\ndef test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\\n    print(\\\"Running test 2\\\")\\n    assert example_data_file_for_session.exists()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\n",
    "    \"\"\"Yield a large file, then delete it after the test completes.\"\"\"\n",
    "    large_data_file = fetch_file_from_s3()\n",
    "    yield large_data_file\n",
    "    print(\"Cleaning up file: {}\".format(large_data_file))\n",
    "    large_data_file.unlink()\n",
    "    \n",
    "    \n",
    "def test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\n",
    "    print(\"Running test 1\")\n",
    "    assert example_data_file_for_session.exists()\n",
    "    \n",
    "def test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\n",
    "    print(\"Running test 2\")\n",
    "    assert example_data_file_for_session.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterising fixtures\n",
    "\n",
    "If you find yourself using the same `pytest.mark.parametrize` multiple times in tests, this can be refactored into the fixutres as `pytest.fixture(params=...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(params=[\\\"\\\", tempfile.mkdtemp(), \\\"/non_existing_file\\\"])\\ndef invalid_file(request) -> pathlib.Path:\\n    return pathlib.Path(request.param)\\n\\ndef test_file_1(invalid_file):\\n    with pytest.raises(FileNotFoundError):\\n        check_input_file(invalid_file)\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\\n        \\ndef test_cli_app(invalid_file):\\n    assert run_cli(invalid_file) == 1\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(params=[\\\"\\\", tempfile.mkdtemp(), \\\"/non_existing_file\\\"])\\ndef invalid_file(request) -> pathlib.Path:\\n    return pathlib.Path(request.param)\\n\\ndef test_file_1(invalid_file):\\n    with pytest.raises(FileNotFoundError):\\n        check_input_file(invalid_file)\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\\n        \\ndef test_cli_app(invalid_file):\\n    assert run_cli(invalid_file) == 1\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes checking for input file: .\n",
      ".Test passes checking for input file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp79iqzr59\n",
      ".Test passes checking for input file: /non_existing_file\n",
      ".Test passes checking for input file: .\n",
      ".Test passes checking for input file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmp79iqzr59\n",
      ".Test passes checking for input file: /non_existing_file\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(params=[\\\"\\\", tempfile.mkdtemp(), \\\"/non_existing_file\\\"])\\ndef invalid_file(request) -> pathlib.Path:\\n    return pathlib.Path(request.param)\\n\\ndef test_file_1(invalid_file):\\n    with pytest.raises(FileNotFoundError):\\n        check_input_file(invalid_file)\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\\n        \\ndef test_cli_app(invalid_file):\\n    assert run_cli(invalid_file) == 1\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(params=[\\\"\\\", tempfile.mkdtemp(), \\\"/non_existing_file\\\"])\\ndef invalid_file(request) -> pathlib.Path:\\n    return pathlib.Path(request.param)\\n\\ndef test_file_1(invalid_file):\\n    with pytest.raises(FileNotFoundError):\\n        check_input_file(invalid_file)\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\\n        \\ndef test_cli_app(invalid_file):\\n    assert run_cli(invalid_file) == 1\\n    print(\\\"Test passes checking for input file: {}\\\".format(invalid_file))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture(params=[\"\", tempfile.mkdtemp(), \"/non_existing_file\"])\n",
    "def invalid_file(request) -> pathlib.Path:\n",
    "    return pathlib.Path(request.param)\n",
    "\n",
    "def test_file_1(invalid_file):\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        check_input_file(invalid_file)\n",
    "    print(\"Test passes checking for input file: {}\".format(invalid_file))\n",
    "        \n",
    "def test_cli_app(invalid_file):\n",
    "    assert run_cli(invalid_file) == 1\n",
    "    print(\"Test passes checking for input file: {}\".format(invalid_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break up long serial tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_long_e2e_test(tmp_path: pathlib.Path):\\n    \\\"\\\"\\\"Long running e2e test.\\\"\\\"\\\"\\n    \\n    # Assume this data was generated from an expensive computation that takes a few minutes \\n    # to run each time.\\n    raw_collected_data = long_running_computation()\\n    \\n    # Here's the raw output\\n    raw_data_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_data_file.write_text(raw_collected_data.to_csv())\\n    \\n    # Here's some computation on the raw output\\n    averages_data_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_data_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n    \\n    # If these tests fail ...\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\n    # ... these won't be executed.\\n    # Which can be brittle and need multiple iterations before all assertions are run.\\n    assert averages_data_file.exists()\\n    assert averages_data_file.read_text()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_long_e2e_test(tmp_path: pathlib.Path):\\n    \\\"\\\"\\\"Long running e2e test.\\\"\\\"\\\"\\n    \\n    # Assume this data was generated from an expensive computation that takes a few minutes \\n    # to run each time.\\n    raw_collected_data = long_running_computation()\\n    \\n    # Here's the raw output\\n    raw_data_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_data_file.write_text(raw_collected_data.to_csv())\\n    \\n    # Here's some computation on the raw output\\n    averages_data_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_data_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n    \\n    # If these tests fail ...\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\n    # ... these won't be executed.\\n    # Which can be brittle and need multiple iterations before all assertions are run.\\n    assert averages_data_file.exists()\\n    assert averages_data_file.read_text()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_long_e2e_test(tmp_path: pathlib.Path):\\n    \\\"\\\"\\\"Long running e2e test.\\\"\\\"\\\"\\n    \\n    # Assume this data was generated from an expensive computation that takes a few minutes \\n    # to run each time.\\n    raw_collected_data = long_running_computation()\\n    \\n    # Here's the raw output\\n    raw_data_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_data_file.write_text(raw_collected_data.to_csv())\\n    \\n    # Here's some computation on the raw output\\n    averages_data_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_data_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n    \\n    # If these tests fail ...\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\n    # ... these won't be executed.\\n    # Which can be brittle and need multiple iterations before all assertions are run.\\n    assert averages_data_file.exists()\\n    assert averages_data_file.read_text()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_long_e2e_test(tmp_path: pathlib.Path):\\n    \\\"\\\"\\\"Long running e2e test.\\\"\\\"\\\"\\n    \\n    # Assume this data was generated from an expensive computation that takes a few minutes \\n    # to run each time.\\n    raw_collected_data = long_running_computation()\\n    \\n    # Here's the raw output\\n    raw_data_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_data_file.write_text(raw_collected_data.to_csv())\\n    \\n    # Here's some computation on the raw output\\n    averages_data_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_data_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n    \\n    # If these tests fail ...\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\n    # ... these won't be executed.\\n    # Which can be brittle and need multiple iterations before all assertions are run.\\n    assert averages_data_file.exists()\\n    assert averages_data_file.read_text()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "def test_long_e2e_test(tmp_path: pathlib.Path):\n",
    "    \"\"\"Long running e2e test.\"\"\"\n",
    "    \n",
    "    # Assume this data was generated from an expensive computation that takes a few minutes \n",
    "    # to run each time.\n",
    "    raw_collected_data = long_running_computation()\n",
    "    \n",
    "    # Here's the raw output\n",
    "    raw_data_file = tmp_path / \"raw_data.csv\"\n",
    "    raw_data_file.write_text(raw_collected_data.to_csv())\n",
    "    \n",
    "    # Here's some computation on the raw output\n",
    "    averages_data_file = tmp_path / \"sample_averages.csv\"\n",
    "    averages_data_file.write_text(\n",
    "        raw_collected_data.groupby([\"sample_id\"]).agg(\"mean\").to_csv()\n",
    "    )\n",
    "    \n",
    "    # If these tests fail ...\n",
    "    assert raw_data_file.exists()\n",
    "    assert raw_data_file.read_text()\n",
    "    \n",
    "    # ... these won't be executed.\n",
    "    # Which can be brittle and need multiple iterations before all assertions are run.\n",
    "    assert averages_data_file.exists()\n",
    "    assert averages_data_file.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with test structure above is that running a lot of tests in serial means the later tests won't execute if any of the earlier ones fail which can require running the same tests multiple times until all the serial tests execute. These can instead be rewritten to take advantage of fixtures and still run all the tests even if some fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n# Move the long running code into a fixture and make sure it runs only once per testing session\\n@pytest.fixture(scope=\\\"session\\\")\\ndef computation_data(tmp_path_factory: pytest.TempPathFactory) -> typing.Dict[str, pathlib.Path]:\\n    # This data was generated by a long compuation.\\n    raw_collected_data = long_running_computation()\\n    \\n    # tmp_path_factory is a fixture provided by pytest: \\n    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\\n    tmp_path = tmp_path_factory.mktemp(\\\"e2e_test\\\")\\n    \\n    # Same generated output files\\n    raw_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_file.write_text(raw_collected_data.to_csv())\\n    averages_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n\\n    # Return the files for testing.\\n    return {\\n        \\\"raw\\\": raw_file,\\n        \\\"averages\\\": averages_file\\n    }\\n\\n\\n# Both these tests use the compuation data as a fixture.\\n# Which means if either test fails, the other tests will still run.\\n# This can also make the tests more modular and easy to read.\\n\\ndef test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    raw_data_file = computation_data[\\\"raw\\\"]\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\ndef test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    averages_file = computation_data[\\\"averages\\\"]\\n    assert averages_file.exists()\\n    assert averages_file.read_text()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n# Move the long running code into a fixture and make sure it runs only once per testing session\\n@pytest.fixture(scope=\\\"session\\\")\\ndef computation_data(tmp_path_factory: pytest.TempPathFactory) -> typing.Dict[str, pathlib.Path]:\\n    # This data was generated by a long compuation.\\n    raw_collected_data = long_running_computation()\\n    \\n    # tmp_path_factory is a fixture provided by pytest: \\n    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\\n    tmp_path = tmp_path_factory.mktemp(\\\"e2e_test\\\")\\n    \\n    # Same generated output files\\n    raw_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_file.write_text(raw_collected_data.to_csv())\\n    averages_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n\\n    # Return the files for testing.\\n    return {\\n        \\\"raw\\\": raw_file,\\n        \\\"averages\\\": averages_file\\n    }\\n\\n\\n# Both these tests use the compuation data as a fixture.\\n# Which means if either test fails, the other tests will still run.\\n# This can also make the tests more modular and easy to read.\\n\\ndef test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    raw_data_file = computation_data[\\\"raw\\\"]\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\ndef test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    averages_file = computation_data[\\\"averages\\\"]\\n    assert averages_file.exists()\\n    assert averages_file.read_text()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n# Move the long running code into a fixture and make sure it runs only once per testing session\\n@pytest.fixture(scope=\\\"session\\\")\\ndef computation_data(tmp_path_factory: pytest.TempPathFactory) -> typing.Dict[str, pathlib.Path]:\\n    # This data was generated by a long compuation.\\n    raw_collected_data = long_running_computation()\\n    \\n    # tmp_path_factory is a fixture provided by pytest: \\n    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\\n    tmp_path = tmp_path_factory.mktemp(\\\"e2e_test\\\")\\n    \\n    # Same generated output files\\n    raw_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_file.write_text(raw_collected_data.to_csv())\\n    averages_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n\\n    # Return the files for testing.\\n    return {\\n        \\\"raw\\\": raw_file,\\n        \\\"averages\\\": averages_file\\n    }\\n\\n\\n# Both these tests use the compuation data as a fixture.\\n# Which means if either test fails, the other tests will still run.\\n# This can also make the tests more modular and easy to read.\\n\\ndef test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    raw_data_file = computation_data[\\\"raw\\\"]\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\ndef test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    averages_file = computation_data[\\\"averages\\\"]\\n    assert averages_file.exists()\\n    assert averages_file.read_text()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n# Move the long running code into a fixture and make sure it runs only once per testing session\\n@pytest.fixture(scope=\\\"session\\\")\\ndef computation_data(tmp_path_factory: pytest.TempPathFactory) -> typing.Dict[str, pathlib.Path]:\\n    # This data was generated by a long compuation.\\n    raw_collected_data = long_running_computation()\\n    \\n    # tmp_path_factory is a fixture provided by pytest: \\n    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\\n    tmp_path = tmp_path_factory.mktemp(\\\"e2e_test\\\")\\n    \\n    # Same generated output files\\n    raw_file = tmp_path / \\\"raw_data.csv\\\"\\n    raw_file.write_text(raw_collected_data.to_csv())\\n    averages_file = tmp_path / \\\"sample_averages.csv\\\"\\n    averages_file.write_text(\\n        raw_collected_data.groupby([\\\"sample_id\\\"]).agg(\\\"mean\\\").to_csv()\\n    )\\n\\n    # Return the files for testing.\\n    return {\\n        \\\"raw\\\": raw_file,\\n        \\\"averages\\\": averages_file\\n    }\\n\\n\\n# Both these tests use the compuation data as a fixture.\\n# Which means if either test fails, the other tests will still run.\\n# This can also make the tests more modular and easy to read.\\n\\ndef test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    raw_data_file = computation_data[\\\"raw\\\"]\\n    assert raw_data_file.exists()\\n    assert raw_data_file.read_text()\\n    \\ndef test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\\n    averages_file = computation_data[\\\"averages\\\"]\\n    assert averages_file.exists()\\n    assert averages_file.read_text()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "# Move the long running code into a fixture and make sure it runs only once per testing session\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def computation_data(tmp_path_factory: pytest.TempPathFactory) -> typing.Dict[str, pathlib.Path]:\n",
    "    # This data was generated by a long compuation.\n",
    "    raw_collected_data = long_running_computation()\n",
    "    \n",
    "    # tmp_path_factory is a fixture provided by pytest: \n",
    "    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\n",
    "    tmp_path = tmp_path_factory.mktemp(\"e2e_test\")\n",
    "    \n",
    "    # Same generated output files\n",
    "    raw_file = tmp_path / \"raw_data.csv\"\n",
    "    raw_file.write_text(raw_collected_data.to_csv())\n",
    "    averages_file = tmp_path / \"sample_averages.csv\"\n",
    "    averages_file.write_text(\n",
    "        raw_collected_data.groupby([\"sample_id\"]).agg(\"mean\").to_csv()\n",
    "    )\n",
    "\n",
    "    # Return the files for testing.\n",
    "    return {\n",
    "        \"raw\": raw_file,\n",
    "        \"averages\": averages_file\n",
    "    }\n",
    "\n",
    "\n",
    "# Both these tests use the compuation data as a fixture.\n",
    "# Which means if either test fails, the other tests will still run.\n",
    "# This can also make the tests more modular and easy to read.\n",
    "\n",
    "def test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\n",
    "    raw_data_file = computation_data[\"raw\"]\n",
    "    assert raw_data_file.exists()\n",
    "    assert raw_data_file.read_text()\n",
    "    \n",
    "def test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\n",
    "    averages_file = computation_data[\"averages\"]\n",
    "    assert averages_file.exists()\n",
    "    assert averages_file.read_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LineMatcher for testing large text\n",
    "\n",
    "The `LineMatcher` helper class provides methods that can reduce testing large text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_large_text():\\n    example_text = textwrap.dedent(\\\"\\\"\\\"\\n        Two roads diverged in a yellow wood,\\n        And sorry I could not travel both\\n        And be one traveler, long I stood\\n        And looked down one as far as I could\\n        To where it bent in the undergrowth;\\n    \\\"\\\"\\\")\\n    \\n    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\\n    \\n    # Check some lines at random\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a yellow wood,\\\"])\\n    \\n     # Check lines exist with a regex\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a .* wood,\\\"])\\n    \\n    # Check lines don't exist with a regex\\n    matcher.no_fnmatch_line([\\\"And looked down two as far as I could\\\"])\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_large_text():\\n    example_text = textwrap.dedent(\\\"\\\"\\\"\\n        Two roads diverged in a yellow wood,\\n        And sorry I could not travel both\\n        And be one traveler, long I stood\\n        And looked down one as far as I could\\n        To where it bent in the undergrowth;\\n    \\\"\\\"\\\")\\n    \\n    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\\n    \\n    # Check some lines at random\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a yellow wood,\\\"])\\n    \\n     # Check lines exist with a regex\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a .* wood,\\\"])\\n    \\n    # Check lines don't exist with a regex\\n    matcher.no_fnmatch_line([\\\"And looked down two as far as I could\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_large_text _______________________________\n",
      "\n",
      "    def test_large_text():\n",
      "        example_text = textwrap.dedent(\"\"\"\n",
      "            Two roads diverged in a yellow wood,\n",
      "            And sorry I could not travel both\n",
      "            And be one traveler, long I stood\n",
      "            And looked down one as far as I could\n",
      "            To where it bent in the undergrowth;\n",
      "        \"\"\")\n",
      "    \n",
      "        matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\n",
      "    \n",
      "        # Check some lines at random\n",
      "        matcher.fnmatch_lines_random([\"Two roads diverged in a yellow wood,\"])\n",
      "    \n",
      "         # Check lines exist with a regex\n",
      ">       matcher.fnmatch_lines_random([\"Two roads diverged in a .* wood,\"])\n",
      "E       Failed: matched:  'Two roads diverged in a yellow wood,'\n",
      "E       line 'Two roads diverged in a .* wood,' not found in output\n",
      "\n",
      "<ipython-input-9-910a10d492f0>:16: Failed\n",
      "=========================== short test summary info ===========================\n",
      "FAILED tmp98dfom4i.py::test_large_text - Failed: matched:  'Two roads diverg...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_large_text():\\n    example_text = textwrap.dedent(\\\"\\\"\\\"\\n        Two roads diverged in a yellow wood,\\n        And sorry I could not travel both\\n        And be one traveler, long I stood\\n        And looked down one as far as I could\\n        To where it bent in the undergrowth;\\n    \\\"\\\"\\\")\\n    \\n    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\\n    \\n    # Check some lines at random\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a yellow wood,\\\"])\\n    \\n     # Check lines exist with a regex\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a .* wood,\\\"])\\n    \\n    # Check lines don't exist with a regex\\n    matcher.no_fnmatch_line([\\\"And looked down two as far as I could\\\"])\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\ndef test_large_text():\\n    example_text = textwrap.dedent(\\\"\\\"\\\"\\n        Two roads diverged in a yellow wood,\\n        And sorry I could not travel both\\n        And be one traveler, long I stood\\n        And looked down one as far as I could\\n        To where it bent in the undergrowth;\\n    \\\"\\\"\\\")\\n    \\n    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\\n    \\n    # Check some lines at random\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a yellow wood,\\\"])\\n    \\n     # Check lines exist with a regex\\n    matcher.fnmatch_lines_random([\\\"Two roads diverged in a .* wood,\\\"])\\n    \\n    # Check lines don't exist with a regex\\n    matcher.no_fnmatch_line([\\\"And looked down two as far as I could\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "def test_large_text():\n",
    "    example_text = textwrap.dedent(\"\"\"\n",
    "        Two roads diverged in a yellow wood,\n",
    "        And sorry I could not travel both\n",
    "        And be one traveler, long I stood\n",
    "        And looked down one as far as I could\n",
    "        To where it bent in the undergrowth;\n",
    "    \"\"\")\n",
    "    \n",
    "    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\n",
    "    \n",
    "    # Check some lines at random\n",
    "    matcher.fnmatch_lines_random([\"Two roads diverged in a yellow wood,\"])\n",
    "    \n",
    "     # Check lines exist with a regex\n",
    "    matcher.fnmatch_lines_random([\"Two roads diverged in a .* wood,\"])\n",
    "    \n",
    "    # Check lines don't exist with a regex\n",
    "    matcher.no_fnmatch_line([\"And looked down two as far as I could\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching large files or computation\n",
    "\n",
    "Documentation: [Cache config][cache]\n",
    "\n",
    "Pytest allows caching expensive operations between test runs such as large computation or fetching data. This can used prevent expensive computations from slowing down tests. The cache can be cleared using the flag: `pytest --cache-clear`.\n",
    "\n",
    "To access the cache the `pytestconfig` fixture needs to be in arguments to a function using the cache, this will be an instance of [`_pytest.config.Config`][config_class]. The caveat to using the `get/set` methods is they have to be JSON serialisable, so in the examples below I covert `pathlib.Path` objects to strings to store in the cache.\n",
    "\n",
    "[cache]: https://docs.pytest.org/en/stable/cache.html#the-new-config-cache-object\n",
    "[config_class]: https://docs.pytest.org/en/latest/reference.html#id35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\\n    \\\"\\\"\\\"Fetch and cache a large file from s3.\\n    \\n    Notes:\\n        If the file is in the cache, return it. If it's not in the cache, \\n        then fetch it, cache it, then return it. This will be cached across \\n        multiple testing sessions.\\n    \\n    \\\"\\\"\\\"\\n    if not (data_file := pytestconfig.cache.get(\\\"file_key\\\", None)): \\n        data_file = fetch_file_from_s3()\\n        pytestconfig.cache.set(\\\"file_key\\\", str(data_file))\\n    else:\\n        print(\\\"Using cached version of file.\\\")\\n        \\n    return pathlib.Path(data_file)\\n \\n    \\ndef test_file_1(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"This test will use the non-cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\\n    \\ndef test_file_2(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"Second time around this will use the cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\\n    \\\"\\\"\\\"Fetch and cache a large file from s3.\\n    \\n    Notes:\\n        If the file is in the cache, return it. If it's not in the cache, \\n        then fetch it, cache it, then return it. This will be cached across \\n        multiple testing sessions.\\n    \\n    \\\"\\\"\\\"\\n    if not (data_file := pytestconfig.cache.get(\\\"file_key\\\", None)): \\n        data_file = fetch_file_from_s3()\\n        pytestconfig.cache.set(\\\"file_key\\\", str(data_file))\\n    else:\\n        print(\\\"Using cached version of file.\\\")\\n        \\n    return pathlib.Path(data_file)\\n \\n    \\ndef test_file_1(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"This test will use the non-cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\\n    \\ndef test_file_2(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"Second time around this will use the cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      "..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\\n    \\\"\\\"\\\"Fetch and cache a large file from s3.\\n    \\n    Notes:\\n        If the file is in the cache, return it. If it's not in the cache, \\n        then fetch it, cache it, then return it. This will be cached across \\n        multiple testing sessions.\\n    \\n    \\\"\\\"\\\"\\n    if not (data_file := pytestconfig.cache.get(\\\"file_key\\\", None)): \\n        data_file = fetch_file_from_s3()\\n        pytestconfig.cache.set(\\\"file_key\\\", str(data_file))\\n    else:\\n        print(\\\"Using cached version of file.\\\")\\n        \\n    return pathlib.Path(data_file)\\n \\n    \\ndef test_file_1(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"This test will use the non-cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\\n    \\ndef test_file_2(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"Second time around this will use the cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\";\n",
       "                var nbb_formatted_code = \"%%run_pytest[clean] -qq -s --cache-clear\\n\\n\\n@pytest.fixture(scope=\\\"session\\\")\\ndef example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\\n    \\\"\\\"\\\"Fetch and cache a large file from s3.\\n    \\n    Notes:\\n        If the file is in the cache, return it. If it's not in the cache, \\n        then fetch it, cache it, then return it. This will be cached across \\n        multiple testing sessions.\\n    \\n    \\\"\\\"\\\"\\n    if not (data_file := pytestconfig.cache.get(\\\"file_key\\\", None)): \\n        data_file = fetch_file_from_s3()\\n        pytestconfig.cache.set(\\\"file_key\\\", str(data_file))\\n    else:\\n        print(\\\"Using cached version of file.\\\")\\n        \\n    return pathlib.Path(data_file)\\n \\n    \\ndef test_file_1(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"This test will use the non-cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\\n    \\ndef test_file_2(example_data_file: pathlib.Path):\\n    \\\"\\\"\\\"Second time around this will use the cached version.\\\"\\\"\\\"\\n    assert example_data_file.exists()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\n",
    "    \"\"\"Fetch and cache a large file from s3.\n",
    "    \n",
    "    Notes:\n",
    "        If the file is in the cache, return it. If it's not in the cache, \n",
    "        then fetch it, cache it, then return it. This will be cached across \n",
    "        multiple testing sessions.\n",
    "    \n",
    "    \"\"\"\n",
    "    if not (data_file := pytestconfig.cache.get(\"file_key\", None)): \n",
    "        data_file = fetch_file_from_s3()\n",
    "        pytestconfig.cache.set(\"file_key\", str(data_file))\n",
    "    else:\n",
    "        print(\"Using cached version of file.\")\n",
    "        \n",
    "    return pathlib.Path(data_file)\n",
    " \n",
    "    \n",
    "def test_file_1(example_data_file: pathlib.Path):\n",
    "    \"\"\"This test will use the non-cached version.\"\"\"\n",
    "    assert example_data_file.exists()\n",
    "    \n",
    "def test_file_2(example_data_file: pathlib.Path):\n",
    "    \"\"\"Second time around this will use the cached version.\"\"\"\n",
    "    assert example_data_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
