{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import typing\n",
    "import textwrap\n",
    "\n",
    "import pandas\n",
    "import pytest\n",
    "import ipytest\n",
    "import _pytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "def check_input_file(input_file: pathlib.Path) -> None:\n",
    "    if not input_file.is_file():\n",
    "        raise FileNotFoundError(\"\")\n",
    "        \n",
    "def run_cli(input_file: pathlib.Path) -> None:\n",
    "    \"\"\"Helper to simulate running a cli tool.\"\"\"\n",
    "    try:\n",
    "        check_input_file(input_file)\n",
    "    except FileNotFoundError:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def long_running_computation():\n",
    "    \"\"\"Helper method to generate some example pandas data\"\"\"\n",
    "    return pandas.DataFrame({\n",
    "        \"sample_id\": [1, 1, 2, 2, 1, 1, 2, 2],\n",
    "        \"measurement\": [0.1, 0.09, 2, 2.3, 5, 4.8, 7.2, 8.3],\n",
    "        \"test_variable\": [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"]      \n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using fixtures to teardown\n",
    "\n",
    "Documentation: [pytest fixture][], [fixture finalization][]\n",
    "\n",
    "[fixture finalization]: https://docs.pytest.org/en/latest/fixture.html#teardown-cleanup-aka-fixture-finalization\n",
    "\n",
    "\n",
    "If you want to do clean up on the fixture you can use `yield` instead of `return`. The fixture will then run which ever code is defined after the `yield`.\n",
    "\n",
    "\n",
    "[pytest_fixture]: https://docs.pytest.org/en/latest/reference.html#pytest-fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmphy29dxt0\n",
      "Fetching a large file from S3.\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmpdampftr3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "@pytest.fixture\n",
    "def example_data_file_with_teardown() -> typing.Generator[pathlib.Path, None, None]:\n",
    "    \"\"\"Yield a large file, then delete it after each test completes.\"\"\"\n",
    "    large_data_file = fetch_file_from_s3()\n",
    "    yield large_data_file\n",
    "    print(\"Cleaning up file: {}\".format(large_data_file))\n",
    "    large_data_file.unlink()\n",
    "    \n",
    "    \n",
    "def test_fixture_teardown_1(example_data_file_with_teardown: pathlib.Path):\n",
    "    assert example_data_file_with_teardown.exists()\n",
    "    \n",
    "def test_fixture_teardown_2(example_data_file_with_teardown: pathlib.Path):\n",
    "    assert example_data_file_with_teardown.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping fixtures\n",
    "\n",
    "Documentation: [scope sharing][]\n",
    "\n",
    "[scope sharing]: https://docs.pytest.org/en/latest/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\n",
    "\n",
    "In the example above the fixture teardown runs every time the fixture is used, this might be inappropriate if the fixture computationally expensive. An alternative to caching described above, would be to set the scope of the fixture with `pytest.fixture(scope=...)`. For example `pytest.fixture(scope=\"session\")` will run once for the pytest session. The values for `scope=...` are `[\"class\", \"module\", \"package\", \"session\"]`. A `Callable` can also be passed which will be evaluated once, see [dynamic scope].\n",
    "\n",
    "[dynamic scope]: https://docs.pytest.org/en/latest/fixture.html#dynamic-scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      "Running test 1\n",
      ".Running test 2\n",
      ".Cleaning up file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmpvm9uvwrr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def example_data_file_for_session() -> typing.Generator[pathlib.Path, None, None]:\n",
    "    \"\"\"Yield a large file, then delete it after the test completes.\"\"\"\n",
    "    large_data_file = fetch_file_from_s3()\n",
    "    yield large_data_file\n",
    "    print(\"Cleaning up file: {}\".format(large_data_file))\n",
    "    large_data_file.unlink()\n",
    "    \n",
    "    \n",
    "def test_fixture__session_teardown_1(example_data_file_for_session: pathlib.Path):\n",
    "    print(\"Running test 1\")\n",
    "    assert example_data_file_for_session.exists()\n",
    "    \n",
    "def test_fixture_session_teardown_2(example_data_file_for_session: pathlib.Path):\n",
    "    print(\"Running test 2\")\n",
    "    assert example_data_file_for_session.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterising fixtures\n",
    "\n",
    "If you find yourself using the same `pytest.mark.parametrize` multiple times in tests, this can be refactored into the fixutres as `pytest.fixture(params=...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes checking for input file: .\n",
      ".Test passes checking for input file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmpb7h6mgx3\n",
      ".Test passes checking for input file: /non_existing_file\n",
      ".Test passes checking for input file: .\n",
      ".Test passes checking for input file: /var/folders/kw/hfflzwfs3xl1_xzq83270dh40000gn/T/tmpb7h6mgx3\n",
      ".Test passes checking for input file: /non_existing_file\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "@pytest.fixture(params=[\"\", tempfile.mkdtemp(), \"/non_existing_file\"])\n",
    "def invalid_file(request) -> pathlib.Path:\n",
    "    return pathlib.Path(request.param)\n",
    "\n",
    "def test_file_1(invalid_file):\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        check_input_file(invalid_file)\n",
    "    print(\"Test passes checking for input file: {}\".format(invalid_file))\n",
    "        \n",
    "def test_cli_app(invalid_file):\n",
    "    assert run_cli(invalid_file) == 1\n",
    "    print(\"Test passes checking for input file: {}\".format(invalid_file))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break up long serial tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "def test_long_e2e_test(tmp_path: pathlib.Path):\n",
    "    \"\"\"Long running e2e test.\"\"\"\n",
    "    \n",
    "    # Assume this data was generated from an expensive computation that takes a few minutes \n",
    "    # to run each time.\n",
    "    raw_collected_data = long_running_computation()\n",
    "    \n",
    "    # Here's the raw output\n",
    "    raw_data_file = tmp_path / \"raw_data.csv\"\n",
    "    raw_data_file.write_text(raw_collected_data.to_csv())\n",
    "    \n",
    "    # Here's some computation on the raw output\n",
    "    averages_data_file = tmp_path / \"sample_averages.csv\"\n",
    "    averages_data_file.write_text(\n",
    "        raw_collected_data.groupby([\"sample_id\"]).agg(\"mean\").to_csv()\n",
    "    )\n",
    "    \n",
    "    # If these tests fail ...\n",
    "    assert raw_data_file.exists()\n",
    "    assert raw_data_file.read_text()\n",
    "    \n",
    "    # ... these won't be executed.\n",
    "    # Which can be brittle and need multiple iterations before all assertions are run.\n",
    "    assert averages_data_file.exists()\n",
    "    assert averages_data_file.read_text()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with test structure above is that running a lot of tests in serial means the later tests won't execute if any of the earlier ones fail which can require running the same tests multiple times until all the serial tests execute. These can instead be rewritten to take advantage of fixtures and still run all the tests even if some fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "# Move the long running code into a fixture and make sure it runs only once per testing session\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def computation_data(tmp_path_factory: pytest.TempPathFactory) -> typing.Dict[str, pathlib.Path]:\n",
    "    # This data was generated by a long compuation.\n",
    "    raw_collected_data = long_running_computation()\n",
    "    \n",
    "    # tmp_path_factory is a fixture provided by pytest: \n",
    "    # https://docs.pytest.org/en/stable/tmpdir.html#tmp-path-factory-example\n",
    "    tmp_path = tmp_path_factory.mktemp(\"e2e_test\")\n",
    "    \n",
    "    # Same generated output files\n",
    "    raw_file = tmp_path / \"raw_data.csv\"\n",
    "    raw_file.write_text(raw_collected_data.to_csv())\n",
    "    averages_file = tmp_path / \"sample_averages.csv\"\n",
    "    averages_file.write_text(\n",
    "        raw_collected_data.groupby([\"sample_id\"]).agg(\"mean\").to_csv()\n",
    "    )\n",
    "\n",
    "    # Return the files for testing.\n",
    "    return {\n",
    "        \"raw\": raw_file,\n",
    "        \"averages\": averages_file\n",
    "    }\n",
    "\n",
    "\n",
    "# Both these tests use the compuation data as a fixture.\n",
    "# Which means if either test fails, the other tests will still run.\n",
    "# This can also make the tests more modular and easy to read.\n",
    "\n",
    "def test_raw_data_file(computation_data: typing.Dict[str, pathlib.Path]):\n",
    "    raw_data_file = computation_data[\"raw\"]\n",
    "    assert raw_data_file.exists()\n",
    "    assert raw_data_file.read_text()\n",
    "    \n",
    "def test_averates_data_file(computation_data: typing.Dict[str, pathlib.Path]):\n",
    "    averages_file = computation_data[\"averages\"]\n",
    "    assert averages_file.exists()\n",
    "    assert averages_file.read_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LineMatcher for testing large text\n",
    "\n",
    "The `LineMatcher` helper class provides methods that can reduce testing large text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "========================================================================== FAILURES ==========================================================================\n",
      "______________________________________________________________________ test_large_text _______________________________________________________________________\n",
      "\n",
      "    def test_large_text():\n",
      "        example_text = textwrap.dedent(\"\"\"\n",
      "            Two roads diverged in a yellow wood,\n",
      "            And sorry I could not travel both\n",
      "            And be one traveler, long I stood\n",
      "            And looked down one as far as I could\n",
      "            To where it bent in the undergrowth;\n",
      "        \"\"\")\n",
      "    \n",
      "        matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\n",
      "    \n",
      "        # Check some lines at random\n",
      "        matcher.fnmatch_lines_random([\"Two roads diverged in a yellow wood,\"])\n",
      "    \n",
      "         # Check lines exist with a regex\n",
      ">       matcher.fnmatch_lines_random([\"Two roads diverged in a .* wood,\"])\n",
      "E       Failed: matched:  'Two roads diverged in a yellow wood,'\n",
      "E       line 'Two roads diverged in a .* wood,' not found in output\n",
      "\n",
      "<ipython-input-26-a227961d30ea>:16: Failed\n",
      "================================================================== short test summary info ===================================================================\n",
      "FAILED tmpcns4tnv1.py::test_large_text - Failed: matched:  'Two roads diverged in a yellow wood,'\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "def test_large_text():\n",
    "    example_text = textwrap.dedent(\"\"\"\n",
    "        Two roads diverged in a yellow wood,\n",
    "        And sorry I could not travel both\n",
    "        And be one traveler, long I stood\n",
    "        And looked down one as far as I could\n",
    "        To where it bent in the undergrowth;\n",
    "    \"\"\")\n",
    "    \n",
    "    matcher = _pytest.pytester.LineMatcher(example_text.splitlines())\n",
    "    \n",
    "    # Check some lines at random\n",
    "    matcher.fnmatch_lines_random([\"Two roads diverged in a yellow wood,\"])\n",
    "    \n",
    "     # Check lines exist with a regex\n",
    "    matcher.fnmatch_lines_random([\"Two roads diverged in a .* wood,\"])\n",
    "    \n",
    "    # Check lines don't exist with a regex\n",
    "    matcher.no_fnmatch_line([\"And looked down two as far as I could\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching large files or computation\n",
    "\n",
    "Documentation: [Cache config][cache]\n",
    "\n",
    "Pytest allows caching expensive operations between test runs such as large computation or fetching data. This can used prevent expensive computations from slowing down tests. The cache can be cleared using the flag: `pytest --cache-clear`.\n",
    "\n",
    "To access the cache the `pytestconfig` fixture needs to be in arguments to a function using the cache, this will be an instance of [`_pytest.config.Config`][config_class]. The caveat to using the `get/set` methods is they have to be JSON serialisable, so in the examples below I covert `pathlib.Path` objects to strings to store in the cache.\n",
    "\n",
    "[cache]: https://docs.pytest.org/en/stable/cache.html#the-new-config-cache-object\n",
    "[config_class]: https://docs.pytest.org/en/latest/reference.html#id35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a large file from S3.\n",
      ".Using cached version of file.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq -s --cache-clear\n",
    "\n",
    "\n",
    "def fetch_file_from_s3() -> pathlib.Path:\n",
    "    \"\"\"Simulate fetching a very large file from s3 that takes a while to download.\"\"\"\n",
    "    print(\"Fetching a large file from S3.\")\n",
    "    _, loc = tempfile.mkstemp()\n",
    "    return pathlib.Path(loc)\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def example_data_file(pytestconfig: _pytest.config.Config) -> pathlib.Path:\n",
    "    \"\"\"Fetch and cache a large file from s3.\n",
    "    \n",
    "    Notes:\n",
    "        If the file is in the cache, return it. If it's not in the cache, \n",
    "        then fetch it, cache it, then return it. This will be cached across \n",
    "        multiple testing sessions.\n",
    "    \n",
    "    \"\"\"\n",
    "    if not (data_file := pytestconfig.cache.get(\"file_key\", None)): \n",
    "        data_file = fetch_file_from_s3()\n",
    "        pytestconfig.cache.set(\"file_key\", str(data_file))\n",
    "    else:\n",
    "        print(\"Using cached version of file.\")\n",
    "        \n",
    "    return pathlib.Path(data_file)\n",
    " \n",
    "    \n",
    "def test_file_1(example_data_file: pathlib.Path):\n",
    "    \"\"\"This test will use the non-cached version.\"\"\"\n",
    "    assert example_data_file.exists()\n",
    "    \n",
    "def test_file_2(example_data_file: pathlib.Path):\n",
    "    \"\"\"Second time around this will use the cached version.\"\"\"\n",
    "    assert example_data_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
